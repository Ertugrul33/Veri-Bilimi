# -*- coding: utf-8 -*-
"""basic_training_loops.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bKiyIPP6DyyiIDCCQeVT0KLtdoywb5TT

## Temel eğitim döngüleri (Training)

TensorFlow ayrıca, standart metnini azaltmak için yararlı soyutlamalar sağlayan yüksek seviyeli bir sinir ağı API'si olan tf.Keras API'yi içerir. Ancak burada temel sınıfları kullanacaksınız.

## Setup
"""

import tensorflow as tf

"""## Solving machine learning problems

Makine öğrenimi sorunlarını çözme
Bir makine öğrenimi problemini çözmek genellikle aşağıdaki adımlardan oluşur:

Eğitim verilerini edinin.

Modeli tanımlayın.

Bir kayıp işlevi tanımlayın.

İdeal değerden kaybı hesaplayarak eğitim verilerini gözden geçirin

Bu kayıp için gradyanları hesaplayın ve değişkenleri verilere uyacak şekilde ayarlamak için bir optimize edici kullanın.

Sonuçlarınızı değerlendirin.

Basit lineer regresyon modeli olarak, f(x) = x * W + b,  2 parametreye sahip $W$ (weights) ve $b$ (bias).

Burada fonksiyonun eğimine bakarak karar veririz.

## Data

Denetimli öğrenme, girdileri (genellikle x olarak gösterilir) ve çıktıları ( y olarak gösterilir, genellikle etiketler olarak adlandırılır) kullanır. Amaç, bir girdiden gelen bir çıktının değerini tahmin edebilmeniz için eşleştirilmiş girdi ve çıktılardan öğrenmektir.

Verilerinizin her girişi, TensorFlow'da, neredeyse her zaman bir tensörle temsil edilir ve genellikle bir vektördür. Denetimli eğitimde çıktı (veya tahmin etmek istediğiniz değer) aynı zamanda bir tensördür.

Bir çizgi boyunca noktalara Gauss (Normal) dağılıma uygun olarak sentezlenen bazı veriler i değerlendirelim
"""

# The actual line
TRUE_W = 3.0
TRUE_B = 2.0

NUM_EXAMPLES = 1000

# A vector of random x values
x = tf.random.normal(shape=[NUM_EXAMPLES])

# Generate some noise
noise = tf.random.normal(shape=[NUM_EXAMPLES])

# Calculate y
y = x * TRUE_W + TRUE_B + noise

# Plot all the data
import matplotlib.pyplot as plt

plt.scatter(x, y, c="b")
plt.show()

"""Tensörler genellikle gruplar halinde veya bir araya yığılmış girdi ve çıktı grupları halinde bir araya getirilir. Gruplama, eğitim anlamında fayda sağlayabilir ve hızlandırıcılar ve vektörleştirilmiş hesaplama ile hızlı çalışır. Bu veri kümesinin ne kadar küçük olduğu göz önüne alındığında, tüm veri kümesini tek bir toplu processing olarak ele alabilirsiniz.

## Modelin belirlenmesi
Bir modeldeki tüm ağırlıkları temsil tf.Variable için tf.Variable kullanın. Bir tf.Variable , bir değeri depolar ve bunu gerektiğinde tensör formunda sağlar. Daha fazla ayrıntı için değişken kılavuzuna bakın.

Değişkenleri ve hesaplamayı kapsüllemek için tf.Module kullanın. Herhangi bir Python nesnesini kullanabilirsiniz, ancak bu şekilde kolayca kaydedilebilir.

Burada, hem w hem de b'yi değişkenler olarak tanımlarsınız.
"""

class MyModel(tf.Module):
  def __init__(self, **kwargs):
    super().__init__(**kwargs)
    # Initialize the weights to `5.0` and the bias to `0.0`
    # In practice, these should be randomly initialized
    self.w = tf.Variable(5.0)
    self.b = tf.Variable(0.0)

  def __call__(self, x):
    return self.w * x + self.b

model = MyModel()

# List the variables tf.modules's built-in variable aggregation.
print("Variables:", model.variables)

# Verify the model works
assert model(3.0).numpy() == 15.0

"""Başlangıç ​​değişkenleri burada sabit bir şekilde ayarlanır, ancak Keras,  bunlar olmadan da kullanabileceğiniz çeşitli başlatıcılarile de devam edebilir.

### Yitim (loss)fonksiyonunun tanımlanması

Bir kayıp işlevi, belirli bir girdi için bir modelin çıktısının hedef çıktıyla ne kadar iyi eşleştiğini ölçer. Amaç, eğitim sırasında bu farkı en aza indirmektir. Ortalama karelerin hatası  MSE olarak da bilinen standart L2 kaybını tanımlayın:
"""

# This computes a single loss value for an entire batch
def loss(target_y, predicted_y):
  return tf.reduce_mean(tf.square(target_y - predicted_y))

"""Before training the model, you can visualize the loss value by plotting the model's predictions in red and the training data in blue:"""

plt.scatter(x, y, c="b")
plt.scatter(x, model(x), c="r")
plt.show()

print("Current loss: %1.6f" % loss(y, model(x)).numpy())

"""### Eğitim döngüsünün tanımlanması 

Eğitim döngüsü, sırayla art arda üç görev yapmaktan oluşur:

Çıktıları oluşturmak için model aracılığıyla bir dizi girdi gönderme

Çıktıları çıktıyla (veya etiketle) karşılaştırarak kaybı hesaplama

Degradeleri bulmak için degrade bant kullanma

Değişkenleri bu gradyanlarla optimize etme

Bu örnek için modeli degrade iniş kullanarak eğitebilirsiniz.

tf.keras.optimizers yakalanan gradyan iniş şemasının birçok çeşidi vardır. Ama ilk prensiplerden bina ruhu içinde, burada yardımıyla temel matematik kendinizi uygulayacaktf.GradientTape otomatik farklılaşma ve için tf.assign_sub (birleştiren bir değer azaltma için tf.assign ve tf.sub )
"""

# Given a callable model, inputs, outputs, and a learning rate...
def train(model, x, y, learning_rate):

  with tf.GradientTape() as t:
    # Trainable variables are automatically tracked by GradientTape
    current_loss = loss(y, model(x))

  # Use GradientTape to calculate the gradients with respect to W and b
  dw, db = t.gradient(current_loss, [model.w, model.b])

  # Subtract the gradient scaled by the learning rate
  model.w.assign_sub(learning_rate * dw)
  model.b.assign_sub(learning_rate * db)

"""Eğitime bir göz atmak için, aynı x ve y grubunu eğitim döngüsünden izleyebilirve W ve b nasıl geliştiğini görebilirsiniz."""

model = MyModel()

# Collect the history of W-values and b-values to plot later
Ws, bs = [], []
epochs = range(10)

# Define a training loop
def training_loop(model, x, y):

  for epoch in epochs:
    # Update the model with the single giant batch
    train(model, x, y, learning_rate=0.1)

    # Track this before I update
    Ws.append(model.w.numpy())
    bs.append(model.b.numpy())
    current_loss = loss(y, model(x))

    print("Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f" %
          (epoch, Ws[-1], bs[-1], current_loss))

print("Starting: W=%1.2f b=%1.2f, loss=%2.5f" %
      (model.w, model.b, loss(y, model(x))))

# Do the training
training_loop(model, x, y)

# Plot it
plt.plot(epochs, Ws, "r",
         epochs, bs, "b")

plt.plot([TRUE_W] * len(epochs), "r--",
         [TRUE_B] * len(epochs), "b--")

plt.legend(["W", "b", "True W", "True b"])
plt.show()

# Visualize how the trained model performs
plt.scatter(x, y, c="b")
plt.scatter(x, model(x), c="r")
plt.show()

print("Current loss: %1.6f" % loss(model(x), y).numpy())

"""## Bu Keras ile nasıl modellenir

Yukarıdaki kodu Keras'taki eşdeğeriyle karşılaştırmak faydalıdır.

tf.keras.Model olarak tanımlanır 
"""

class MyModelKeras(tf.keras.Model):
  def __init__(self, **kwargs):
    super().__init__(**kwargs)
    # Initialize the weights to `5.0` and the bias to `0.0`
    # In practice, these should be randomly initialized
    self.w = tf.Variable(5.0)
    self.b = tf.Variable(0.0)

  def call(self, x):
    return self.w * x + self.b

keras_model = MyModelKeras()

# Reuse the training loop with a Keras model
training_loop(keras_model, x, y)

# You can also save a checkpoint using Keras's built-in support
keras_model.save_weights("my_checkpoint")

"""Her model oluşturduğunuzda yeni eğitim döngüleri yazmak yerine, Keras'ın yerleşik özelliklerini kısayol olarak kullanabilirsiniz. Python eğitim döngüleri yazmak veya hata ayıklamak istemediğinizde bu yararlı olabilir.

Bunu yaparsanız, parametreleri ayarlamak ve eğitmek için model.fit()  model.compile() kullanmanız ile de derleme yapılır  . L2 kaybının ve gradyan inişinin Keras uygulamalarını yine bir kısayol olarak kullanmak daha az kod ile mümkün olabilir. Loss fonk ve iyileştiricileri, bu kolaylık işlevlerinin dışında da kullanılabilir
"""

keras_model = MyModelKeras()

# compile sets the training parameters
keras_model.compile(
    # By default, fit() uses tf.function().  You can
    # turn that off for debugging, but it is on now.
    run_eagerly=False,

    # Using a built-in optimizer, configuring as an object
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),

    # Keras comes with built-in MSE error
    # However, you could use the loss function
    # defined above
    loss=tf.keras.losses.mean_squared_error,
)

"""Keras fit fonksiyonu NumPy dizisi olarak bekler. NumPy dizileri gruplar halinde kesilir ve varsayılan olarak 32 toplu iş boyutuna ayarlanır.

"""

print(x.shape[0])
keras_model.fit(x, y, epochs=10, batch_size=1000)